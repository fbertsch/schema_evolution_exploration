{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to express the semantics of schema evolution of parquet files using both Spark and parquet2hive + Presto. Note that all parquet2hive is doing is reading the schema *from the most recently created file*, so in some cases this could be changed without changing Presto's underlying facilities by reading *all* files.\n",
    "\n",
    "In this notebook I am running Spark locally, and using a remote Presto cluster. To connect to this cluster I'm using parquet2hive_server [0], which is just a simple API for parquet2hive on the remote cluster. To run this notebook successfully, you'll need to run the following on the Presto cluster:\n",
    "\n",
    "```\n",
    "sudo pip install parquet2hive_server\n",
    "start_parquet2hive_server\n",
    "```\n",
    "\n",
    "[0] http://www.github.com/fbertsch/parquet2hive_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parquet2hive_server.client import Parquet2HiveClient\n",
    "from pyhive import presto\n",
    "from pprint import pprint\n",
    "\n",
    "presto_dns = 'ec2-54-149-100-125.us-west-2.compute.amazonaws.com'\n",
    "\n",
    "client = Parquet2HiveClient(presto_dns + ':5129')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket, prefix = \"telemetry-test-bucket\", \"schema_evolution\"\n",
    "dataset = \"s3://{}/{}/\".format(bucket, prefix)\n",
    "partition = '/type='\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "objects_to_delete = s3.meta.client.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "delete_keys = {}\n",
    "delete_keys['Objects'] = [{'Key' : k} for k in [obj['Key'] for obj in objects_to_delete.get('Contents', [])]]\n",
    "\n",
    "try:\n",
    "    _ = s3.meta.client.delete_objects(Bucket=bucket, Delete=delete_keys)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = presto.connect(host=presto_dns, port='8889')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def get_schema(_cursor, _v):\n",
    "    \"\"\"Prints the schema in a similar format to spark's dataframe.printSchema()\"\"\"\n",
    "    _cursor.execute('describe schema_evolution_{}'.format(_v))\n",
    "    return '\\\\\\n'.join(['root'] + [' |-- {}: {}'.format(a, b) for a, b, _, _ in _cursor.fetchall()])\n",
    "\n",
    "def execute(_cursor, _query):\n",
    "    _cursor.execute(_query)\n",
    "    results = _cursor.fetchall()\n",
    "    colnames = [c[0] for c in _cursor.description]\n",
    "    return '\\\\\\n'.join([', '.join(['{}={}'.format(c,r) for c,r in zip(colnames, res)]) for res in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,0),(1,1)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'score'])\n",
    "df.write.parquet(dataset + v + partition  + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, type=u'2'),\n",
       " Row(id=1, type=u'2'),\n",
       " Row(id=0, type=u'1'),\n",
       " Row(id=1, type=u'1')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'2'),\n",
       " Row(id=1, score=1, type=u'2'),\n",
       " Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- score: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, score=0, type=2\\\n",
      "id=1, score=1, type=2\\\n",
      "id=0, score=None, type=1\\\n",
      "id=1, score=None, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v2'\n",
    "\n",
    "rdd = sc.parallelize([(0,0),(1,1)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'score'])\n",
    "df.write.parquet(dataset + v + partition  + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'1'),\n",
       " Row(id=1, score=1, type=u'1'),\n",
       " Row(id=0, score=None, type=u'2'),\n",
       " Row(id=1, score=None, type=u'2')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'1'),\n",
       " Row(id=1, score=1, type=u'1'),\n",
       " Row(id=0, score=None, type=u'2'),\n",
       " Row(id=1, score=None, type=u'2')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, type=1\\\n",
      "id=1, type=1\\\n",
      "id=0, type=2\\\n",
      "id=1, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['score'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=None, type=u'2'),\n",
       " Row(id=None, type=u'2'),\n",
       " Row(id=0, type=u'1'),\n",
       " Row(id=1, type=u'1')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=None, score=0, type=u'2'),\n",
       " Row(id=None, score=1, type=u'2'),\n",
       " Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- score: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0, type=2\\\n",
      "score=1, type=2\\\n",
      "score=0, type=1\\\n",
      "score=1, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is similar to \"rename column\", but the new data has a different type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v4'\n",
    "\n",
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',),('b',)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['score'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1'),\n",
       " Row(id=None, score=u'a', type=u'2'),\n",
       " Row(id=None, score=u'b', type=u'2')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- score: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "{u'errorCode': 65536, u'message': u'com.facebook.presto.spi.type.VarcharType', u'errorType': u'INTERNAL_ERROR', u'failureInfo': {u'suppressed': [], u'message': u'com.facebook.presto.spi.type.VarcharType', u'type': u'java.lang.UnsupportedOperationException', u'stack': [u'com.facebook.presto.spi.type.AbstractType.writeLong(AbstractType.java:111)', u'com.facebook.presto.hive.parquet.reader.ParquetLongColumnReader.readValue(ParquetLongColumnReader.java:32)', u'com.facebook.presto.hive.parquet.reader.ParquetColumnReader.readValues(ParquetColumnReader.java:173)', u'com.facebook.presto.hive.parquet.reader.ParquetColumnReader.readPrimitive(ParquetColumnReader.java:155)', u'com.facebook.presto.hive.parquet.reader.ParquetReader.readPrimitive(ParquetReader.java:239)', u'com.facebook.presto.hive.parquet.reader.ParquetReader.readPrimitive(ParquetReader.java:221)', u'com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:283)', u'com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:259)', u'com.facebook.presto.spi.block.LazyBlock.assureLoaded(LazyBlock.java:235)', u'com.facebook.presto.spi.Page.assureLoaded(Page.java:230)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:259)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'errorName': u'GENERIC_INTERNAL_ERROR'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-fa9b7a3812d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SELECT * FROM schema_evolution_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-989af1b253ae>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(_cursor, _query)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcolnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'\\\\\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36mfetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36mfetchone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Sleep until we're done or we have some data to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STATE_FINISHED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36m_fetch_while\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/presto.pyc\u001b[0m in \u001b[0;36m_fetch_more\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;34m\"\"\"Fetch the next URI and update state\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nextUri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decode_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/presto.pyc\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nextUri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should not have nextUri if failed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: {u'errorCode': 65536, u'message': u'com.facebook.presto.spi.type.VarcharType', u'errorType': u'INTERNAL_ERROR', u'failureInfo': {u'suppressed': [], u'message': u'com.facebook.presto.spi.type.VarcharType', u'type': u'java.lang.UnsupportedOperationException', u'stack': [u'com.facebook.presto.spi.type.AbstractType.writeLong(AbstractType.java:111)', u'com.facebook.presto.hive.parquet.reader.ParquetLongColumnReader.readValue(ParquetLongColumnReader.java:32)', u'com.facebook.presto.hive.parquet.reader.ParquetColumnReader.readValues(ParquetColumnReader.java:173)', u'com.facebook.presto.hive.parquet.reader.ParquetColumnReader.readPrimitive(ParquetColumnReader.java:155)', u'com.facebook.presto.hive.parquet.reader.ParquetReader.readPrimitive(ParquetReader.java:239)', u'com.facebook.presto.hive.parquet.reader.ParquetReader.readPrimitive(ParquetReader.java:221)', u'com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:283)', u'com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:259)', u'com.facebook.presto.spi.block.LazyBlock.assureLoaded(LazyBlock.java:235)', u'com.facebook.presto.spi.Page.assureLoaded(Page.java:230)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:259)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'errorName': u'GENERIC_INTERNAL_ERROR'}"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranpose Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v5'\n",
    "\n",
    "rdd = sc.parallelize([(0,'a','b')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transpose_a', 'transpose_b'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,'b','a')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transpose_b', 'transpose_a'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- transpose_a: string (nullable = true)\n",
      " |-- transpose_b: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(transpose_a=u'a', transpose_b=u'b'),\n",
       " Row(transpose_a=u'a', transpose_b=u'b')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').select('transpose_a','transpose_b').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- transpose_a: string (nullable = true)\n",
      " |-- transpose_b: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(transpose_a=u'a', transpose_b=u'b'),\n",
       " Row(transpose_a=u'a', transpose_b=u'b')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').select('transpose_a','transpose_b').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- transpose_b: varchar\\\n",
      " |-- transpose_a: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1, transpose_b=b, transpose_a=a, type=2\\\n",
      "id=0, transpose_b=a, transpose_a=b, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose, Delete and Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v6'\n",
    "\n",
    "rdd = sc.parallelize([(0,'r','t')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'removed', 'transposed'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,'t','a')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transposed', 'added'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- removed: string (nullable = true)\n",
      " |-- transposed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, removed=u'r', transposed=u't', type=u'1'),\n",
       " Row(id=1, removed=None, transposed=u't', type=u'2')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- removed: string (nullable = true)\n",
      " |-- transposed: string (nullable = true)\n",
      " |-- added: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, removed=u'r', transposed=u't', added=None, type=u'1'),\n",
       " Row(id=1, removed=None, transposed=u't', added=u'a', type=u'2')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- transposed: varchar\\\n",
      " |-- added: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, transposed=r, added=t, type=1\\\n",
      "id=1, transposed=t, added=a, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Row Type - Adding a Subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "v = 'v7'\n",
    "\n",
    "df = sqlContext.createDataFrame([[[1, 'e']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[[1, 'e', 'a']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType()),\n",
    "                        StructField(\"added\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e'), type=u'2'),\n",
       " Row(nested=Row(id=1, exists=u'e'), type=u'1')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- added: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', added=u'a'), type=u'2'),\n",
       " Row(nested=Row(id=1, exists=u'e', added=None), type=u'1')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- nested: row(id bigint, exists varchar, added varchar)\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested=[1, u'e', u'a'], type=2\\\n",
      "nested=[1, u'e', None], type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Row Type - Removing a Subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v8'\n",
    "\n",
    "df = sqlContext.createDataFrame([[[1, 'e', 'r']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType()),\n",
    "                        StructField(\"removed\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[[1, 'e']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- removed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', removed=u'r'), type=u'1'),\n",
       " Row(nested=Row(id=1, exists=u'e', removed=None), type=u'2')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- removed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', removed=u'r'), type=u'1'),\n",
       " Row(nested=Row(id=1, exists=u'e', removed=None), type=u'2')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- nested: row(id bigint, exists varchar)\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested=[1, u'e'], type=2\\\n",
      "nested=[1, u'e'], type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
