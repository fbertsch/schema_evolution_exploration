{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to express the semantics of schema evolution of parquet files using both Spark and parquet2hive + Presto. Note that all parquet2hive is doing is reading the schema *from the most recently created file*, so in some cases this could be changed without changing Presto's underlying facilities by reading *all* files.\n",
    "\n",
    "In this notebook I am running Spark locally, and using a remote Presto cluster. To connect to this cluster I'm using parquet2hive_server [0], which is just a simple API for parquet2hive on the remote cluster. To run this notebook successfully, you'll need to run the following on the Presto cluster:\n",
    "\n",
    "```\n",
    "sudo pip install parquet2hive_server\n",
    "start_parquet2hive_server\n",
    "```\n",
    "\n",
    "[0] http://www.github.com/fbertsch/parquet2hive_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parquet2hive_server.client import Parquet2HiveClient\n",
    "from pyhive import presto\n",
    "from pprint import pprint\n",
    "\n",
    "presto_dns = 'ec2-54-149-100-125.us-west-2.compute.amazonaws.com'\n",
    "\n",
    "client = Parquet2HiveClient(presto_dns + ':5129')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket, prefix = \"telemetry-test-bucket\", \"schema_evolution\"\n",
    "dataset = \"s3://{}/{}/\".format(bucket, prefix)\n",
    "partition = '/type='\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "objects_to_delete = s3.meta.client.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "delete_keys = {}\n",
    "delete_keys['Objects'] = [{'Key' : k} for k in [obj['Key'] for obj in objects_to_delete.get('Contents', [])]]\n",
    "\n",
    "try:\n",
    "    _ = s3.meta.client.delete_objects(Bucket=bucket, Delete=delete_keys)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = presto.connect(host=presto_dns, port='8889')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def get_schema(_cursor, _v):\n",
    "    \"\"\"Prints the schema in a similar format to spark's dataframe.printSchema()\"\"\"\n",
    "    _cursor.execute('describe schema_evolution_{}'.format(_v))\n",
    "    return '\\\\\\n'.join(['root'] + [' |-- {}: {}'.format(a, b) for a, b, _, _ in _cursor.fetchall()])\n",
    "\n",
    "def execute(_cursor, _query):\n",
    "    _cursor.execute(_query)\n",
    "    results = _cursor.fetchall()\n",
    "    colnames = [c[0] for c in _cursor.description]\n",
    "    return '\\\\\\n'.join([', '.join(['{}={}'.format(c,r) for c,r in zip(colnames, res)]) for res in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,0),(1,1)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'score'])\n",
    "df.write.parquet(dataset + v + partition  + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, type=u'2'),\n",
       " Row(id=1, type=u'2'),\n",
       " Row(id=0, type=u'1'),\n",
       " Row(id=1, type=u'1')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'2'),\n",
       " Row(id=1, score=1, type=u'2'),\n",
       " Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- score: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, score=None, type=1\\\n",
      "id=1, score=None, type=1\\\n",
      "id=0, score=0, type=2\\\n",
      "id=1, score=1, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v2'\n",
    "\n",
    "rdd = sc.parallelize([(0,0),(1,1)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'score'])\n",
    "df.write.parquet(dataset + v + partition  + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'1'),\n",
       " Row(id=1, score=1, type=u'1'),\n",
       " Row(id=0, score=None, type=u'2'),\n",
       " Row(id=1, score=None, type=u'2')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'1'),\n",
       " Row(id=1, score=1, type=u'1'),\n",
       " Row(id=0, score=None, type=u'2'),\n",
       " Row(id=1, score=None, type=u'2')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, type=2\\\n",
      "id=1, type=2\\\n",
      "id=0, type=1\\\n",
      "id=1, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['score'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=None, type=u'2'),\n",
       " Row(id=None, type=u'2'),\n",
       " Row(id=0, type=u'1'),\n",
       " Row(id=1, type=u'1')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=None, score=0, type=u'2'),\n",
       " Row(id=None, score=1, type=u'2'),\n",
       " Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- score: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=None, type=1\\\n",
      "score=None, type=1\\\n",
      "score=0, type=2\\\n",
      "score=1, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is similar to \"rename column\", but the new data has a different type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v4'\n",
    "\n",
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',),('b',)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['score'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1'),\n",
       " Row(id=None, score=u'a', type=u'2'),\n",
       " Row(id=None, score=u'b', type=u'2')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- score: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=None, type=1\\\n",
      "score=None, type=1\\\n",
      "score=a, type=2\\\n",
      "score=b, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranpose Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v5'\n",
    "\n",
    "rdd = sc.parallelize([(0,'a','b')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transpose_a', 'transpose_b'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,'b','a')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transpose_b', 'transpose_a'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- transpose_a: string (nullable = true)\n",
      " |-- transpose_b: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(transpose_a=u'a', transpose_b=u'b'),\n",
       " Row(transpose_a=u'a', transpose_b=u'b')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').select('transpose_a','transpose_b').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- transpose_a: string (nullable = true)\n",
      " |-- transpose_b: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(transpose_a=u'a', transpose_b=u'b'),\n",
       " Row(transpose_a=u'a', transpose_b=u'b')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').select('transpose_a','transpose_b').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- transpose_b: varchar\\\n",
      " |-- transpose_a: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1, transpose_b=b, transpose_a=a, type=2\\\n",
      "id=0, transpose_b=b, transpose_a=a, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose, Delete and Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v6'\n",
    "\n",
    "rdd = sc.parallelize([(0,'r','t')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'removed', 'transposed'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,'t','a')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transposed', 'added'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- removed: string (nullable = true)\n",
      " |-- transposed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, removed=u'r', transposed=u't', type=u'1'),\n",
       " Row(id=1, removed=None, transposed=u't', type=u'2')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- removed: string (nullable = true)\n",
      " |-- transposed: string (nullable = true)\n",
      " |-- added: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, removed=u'r', transposed=u't', added=None, type=u'1'),\n",
       " Row(id=1, removed=None, transposed=u't', added=u'a', type=u'2')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- transposed: varchar\\\n",
      " |-- added: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1, transposed=t, added=a, type=2\\\n",
      "id=0, transposed=t, added=None, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Row Type - Adding a Subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "v = 'v7'\n",
    "\n",
    "df = sqlContext.createDataFrame([[[1, 'e']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[[1, 'e', 'a']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType()),\n",
    "                        StructField(\"added\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e'), type=u'2'),\n",
       " Row(nested=Row(id=1, exists=u'e'), type=u'1')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- added: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', added=u'a'), type=u'2'),\n",
       " Row(nested=Row(id=1, exists=u'e', added=None), type=u'1')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- nested: row(id bigint, exists varchar, added varchar)\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested=[1, u'e', None], type=1\\\n",
      "nested=[1, u'e', u'a'], type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Row Type - Removing a Subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v8'\n",
    "\n",
    "df = sqlContext.createDataFrame([[[1, 'e', 'r']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType()),\n",
    "                        StructField(\"removed\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[[1, 'e']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- removed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', removed=u'r'), type=u'1'),\n",
       " Row(nested=Row(id=1, exists=u'e', removed=None), type=u'2')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- removed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', removed=u'r'), type=u'1'),\n",
       " Row(nested=Row(id=1, exists=u'e', removed=None), type=u'2')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- nested: row(id bigint, exists varchar)\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested=[1, u'e'], type=1\\\n",
      "nested=[1, u'e'], type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
