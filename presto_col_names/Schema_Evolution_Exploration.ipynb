{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to express the semantics of schema evolution of parquet files using both Spark and parquet2hive + Presto. Note that all parquet2hive is doing is reading the schema *from the most recently created file*, so in some cases this could be changed without changing Presto's underlying facilities by reading *all* files.\n",
    "\n",
    "In this notebook I am running Spark locally, and using a remote Presto cluster. To connect to this cluster I'm using parquet2hive_server [0], which is just a simple API for parquet2hive on the remote cluster. To run this notebook successfully, you'll need to run the following on the Presto cluster:\n",
    "\n",
    "```\n",
    "sudo pip install parquet2hive_server\n",
    "start_parquet2hive_server\n",
    "```\n",
    "\n",
    "[0] http://www.github.com/fbertsch/parquet2hive_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parquet2hive_server.client import Parquet2HiveClient\n",
    "from pyhive import presto\n",
    "from pprint import pprint\n",
    "\n",
    "presto_dns = 'ec2-54-149-100-125.us-west-2.compute.amazonaws.com'\n",
    "\n",
    "client = Parquet2HiveClient(presto_dns + ':5129')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket, prefix = \"telemetry-test-bucket\", \"schema_evolution\"\n",
    "dataset = \"s3://{}/{}/\".format(bucket, prefix)\n",
    "partition = '/type='\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "objects_to_delete = s3.meta.client.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "delete_keys = {}\n",
    "delete_keys['Objects'] = [{'Key' : k} for k in [obj['Key'] for obj in objects_to_delete.get('Contents', [])]]\n",
    "\n",
    "try:\n",
    "    _ = s3.meta.client.delete_objects(Bucket=bucket, Delete=delete_keys)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = presto.connect(host=presto_dns, port='8889')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def get_schema(_cursor, _v):\n",
    "    \"\"\"Prints the schema in a similar format to spark's dataframe.printSchema()\"\"\"\n",
    "    _cursor.execute('describe schema_evolution_{}'.format(_v))\n",
    "    return '\\\\\\n'.join(['root'] + [' |-- {}: {}'.format(a, b) for a, b, _, _ in _cursor.fetchall()])\n",
    "\n",
    "def execute(_cursor, _query):\n",
    "    _cursor.execute(_query)\n",
    "    results = _cursor.fetchall()\n",
    "    colnames = [c[0] for c in _cursor.description]\n",
    "    return '\\\\\\n'.join([', '.join(['{}={}'.format(c,r) for c,r in zip(colnames, res)]) for res in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,0),(1,1)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'score'])\n",
    "df.write.parquet(dataset + v + partition  + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, type=u'2'),\n",
       " Row(id=1, type=u'2'),\n",
       " Row(id=0, type=u'1'),\n",
       " Row(id=1, type=u'1')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'2'),\n",
       " Row(id=1, score=1, type=u'2'),\n",
       " Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- score: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, score=None, type=1\\\n",
      "id=1, score=None, type=1\\\n",
      "id=0, score=0, type=2\\\n",
      "id=1, score=1, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v2'\n",
    "\n",
    "rdd = sc.parallelize([(0,0),(1,1)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'score'])\n",
    "df.write.parquet(dataset + v + partition  + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'1'),\n",
       " Row(id=1, score=1, type=u'1'),\n",
       " Row(id=0, score=None, type=u'2'),\n",
       " Row(id=1, score=None, type=u'2')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=0, type=u'1'),\n",
       " Row(id=1, score=1, type=u'1'),\n",
       " Row(id=0, score=None, type=u'2'),\n",
       " Row(id=1, score=None, type=u'2')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, type=1\\\n",
      "id=1, type=1\\\n",
      "id=0, type=2\\\n",
      "id=1, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['score'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=None, type=u'2'),\n",
       " Row(id=None, type=u'2'),\n",
       " Row(id=0, type=u'1'),\n",
       " Row(id=1, type=u'1')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=None, score=0, type=u'2'),\n",
       " Row(id=None, score=1, type=u'2'),\n",
       " Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- score: bigint\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=0, type=2\\\n",
      "score=1, type=2\\\n",
      "score=None, type=1\\\n",
      "score=None, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is similar to \"rename column\", but the new data has a different type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v4'\n",
    "\n",
    "rdd = sc.parallelize([(0,),(1,)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',),('b',)], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['score'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, score=None, type=u'1'),\n",
       " Row(id=1, score=None, type=u'1'),\n",
       " Row(id=None, score=u'a', type=u'2'),\n",
       " Row(id=None, score=u'b', type=u'2')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- score: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print get_schema(cursor, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score=None, type=1\\\n",
      "score=None, type=1\\\n",
      "score=a, type=2\\\n",
      "score=b, type=2\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranpose Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v5'\n",
    "\n",
    "rdd = sc.parallelize([(0,'a','b')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transpose_a', 'transpose_b'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,'b','a')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transpose_b', 'transpose_a'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- transpose_a: string (nullable = true)\n",
      " |-- transpose_b: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(transpose_a=u'a', transpose_b=u'b'),\n",
       " Row(transpose_a=u'a', transpose_b=u'b')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').select('transpose_a','transpose_b').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- transpose_a: string (nullable = true)\n",
      " |-- transpose_b: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(transpose_a=u'a', transpose_b=u'b'),\n",
       " Row(transpose_a=u'a', transpose_b=u'b')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').select('transpose_a','transpose_b').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- transpose_b: varchar\\\n",
      " |-- transpose_a: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1, transpose_b=b, transpose_a=a, type=2\\\n",
      "id=0, transpose_b=b, transpose_a=a, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose, Delete and Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = 'v6'\n",
    "\n",
    "rdd = sc.parallelize([(0,'r','t')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'removed', 'transposed'])\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,'t','a')], 1)\n",
    "df = sqlContext.createDataFrame(rdd, ['id', 'transposed', 'added'])\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- removed: string (nullable = true)\n",
      " |-- transposed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, removed=u'r', transposed=u't', type=u'1'),\n",
       " Row(id=1, removed=None, transposed=u't', type=u'2')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- removed: string (nullable = true)\n",
      " |-- transposed: string (nullable = true)\n",
      " |-- added: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, removed=u'r', transposed=u't', added=None, type=u'1'),\n",
       " Row(id=1, removed=None, transposed=u't', added=u'a', type=u'2')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- id: bigint\\\n",
      " |-- transposed: varchar\\\n",
      " |-- added: varchar\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1, transposed=t, added=a, type=2\\\n",
      "id=0, transposed=t, added=None, type=1\n"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Row Type - Adding a Subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "v = 'v7'\n",
    "\n",
    "df = sqlContext.createDataFrame([[[1, 'e']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[[1, 'e', 'a']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType()),\n",
    "                        StructField(\"added\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e'), type=u'2'),\n",
       " Row(nested=Row(id=1, exists=u'e'), type=u'1')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- added: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', added=u'a'), type=u'2'),\n",
       " Row(nested=Row(id=1, exists=u'e', added=None), type=u'1')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- nested: row(id bigint, exists varchar, added varchar)\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "{u'errorCode': 16777219, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v7/type=1/part-r-00000-d4702419-81ea-43cf-b8e0-6b89bac2df96.snappy.parquet (offset=0, length=419): Schema mismatch, metastore schema for row column nested has 3 fields but parquet schema has 2 fields', u'errorType': u'EXTERNAL', u'failureInfo': {u'suppressed': [], u'cause': {u'suppressed': [], u'message': u'Schema mismatch, metastore schema for row column nested has 3 fields but parquet schema has 2 fields', u'type': u'java.lang.IllegalArgumentException', u'stack': [u'com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$ParquetStructConverter.<init>(ParquetHiveRecordCursor.java:743)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createGroupConverter(ParquetHiveRecordCursor.java:718)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.access$300(ParquetHiveRecordCursor.java:99)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$PrestoReadSupport.<init>(ParquetHiveRecordCursor.java:434)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:336)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v7/type=1/part-r-00000-d4702419-81ea-43cf-b8e0-6b89bac2df96.snappy.parquet (offset=0, length=419): Schema mismatch, metastore schema for row column nested has 3 fields but parquet schema has 2 fields', u'type': u'com.facebook.presto.spi.PrestoException', u'stack': [u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:382)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'errorName': u'HIVE_CANNOT_OPEN_SPLIT'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-fa9b7a3812d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SELECT * FROM schema_evolution_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-989af1b253ae>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(_cursor, _query)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcolnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'\\\\\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36mfetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36mfetchone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Sleep until we're done or we have some data to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STATE_FINISHED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36m_fetch_while\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/presto.pyc\u001b[0m in \u001b[0;36m_fetch_more\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;34m\"\"\"Fetch the next URI and update state\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nextUri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decode_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/presto.pyc\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nextUri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should not have nextUri if failed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: {u'errorCode': 16777219, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v7/type=1/part-r-00000-d4702419-81ea-43cf-b8e0-6b89bac2df96.snappy.parquet (offset=0, length=419): Schema mismatch, metastore schema for row column nested has 3 fields but parquet schema has 2 fields', u'errorType': u'EXTERNAL', u'failureInfo': {u'suppressed': [], u'cause': {u'suppressed': [], u'message': u'Schema mismatch, metastore schema for row column nested has 3 fields but parquet schema has 2 fields', u'type': u'java.lang.IllegalArgumentException', u'stack': [u'com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$ParquetStructConverter.<init>(ParquetHiveRecordCursor.java:743)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createGroupConverter(ParquetHiveRecordCursor.java:718)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.access$300(ParquetHiveRecordCursor.java:99)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$PrestoReadSupport.<init>(ParquetHiveRecordCursor.java:434)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:336)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v7/type=1/part-r-00000-d4702419-81ea-43cf-b8e0-6b89bac2df96.snappy.parquet (offset=0, length=419): Schema mismatch, metastore schema for row column nested has 3 fields but parquet schema has 2 fields', u'type': u'com.facebook.presto.spi.PrestoException', u'stack': [u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:382)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'errorName': u'HIVE_CANNOT_OPEN_SPLIT'}"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Row Type - Removing a Subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = 'v8'\n",
    "\n",
    "df = sqlContext.createDataFrame([[[1, 'e', 'r']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType()),\n",
    "                        StructField(\"removed\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[[1, 'e']]], StructType([\n",
    "    StructField(\"nested\", \n",
    "                StructType([\n",
    "                        StructField(\"id\", LongType()), \n",
    "                        StructField(\"exists\", StringType())\n",
    "                    ])\n",
    "               )\n",
    "]))\n",
    "\n",
    "df.write.parquet(dataset + v + partition + '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- removed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', removed=u'r'), type=u'1'),\n",
       " Row(nested=Row(id=1, exists=u'e', removed=None), type=u'2')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nested: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- exists: string (nullable = true)\n",
      " |    |-- removed: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nested=Row(id=1, exists=u'e', removed=u'r'), type=u'1'),\n",
       " Row(nested=Row(id=1, exists=u'e', removed=None), type=u'2')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.option(\"mergeSchema\", \"true\").load(dataset + v, 'parquet').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, u'{\"Result\": [null, null]}\\n')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.load(dataset=dataset, dv=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\\\n",
      " |-- nested: row(id bigint, exists varchar)\\\n",
      " |-- type: varchar\n"
     ]
    }
   ],
   "source": [
    "print(get_schema(cursor, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "{u'errorCode': 16777219, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v8/type=1/part-r-00024-7eaefa5c-abc7-419a-94a5-081e2c608256.snappy.parquet (offset=0, length=500): Schema mismatch, metastore schema for row column nested has 2 fields but parquet schema has 3 fields', u'errorType': u'EXTERNAL', u'failureInfo': {u'suppressed': [], u'cause': {u'suppressed': [], u'message': u'Schema mismatch, metastore schema for row column nested has 2 fields but parquet schema has 3 fields', u'type': u'java.lang.IllegalArgumentException', u'stack': [u'com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$ParquetStructConverter.<init>(ParquetHiveRecordCursor.java:743)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createGroupConverter(ParquetHiveRecordCursor.java:718)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.access$300(ParquetHiveRecordCursor.java:99)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$PrestoReadSupport.<init>(ParquetHiveRecordCursor.java:434)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:336)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v8/type=1/part-r-00024-7eaefa5c-abc7-419a-94a5-081e2c608256.snappy.parquet (offset=0, length=500): Schema mismatch, metastore schema for row column nested has 2 fields but parquet schema has 3 fields', u'type': u'com.facebook.presto.spi.PrestoException', u'stack': [u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:382)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'errorName': u'HIVE_CANNOT_OPEN_SPLIT'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-fa9b7a3812d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SELECT * FROM schema_evolution_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-989af1b253ae>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(_cursor, _query)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcolnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_cursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'\\\\\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36mfetchall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36mfetchone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Sleep until we're done or we have some data to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STATE_FINISHED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/common.pyc\u001b[0m in \u001b[0;36m_fetch_while\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/presto.pyc\u001b[0m in \u001b[0;36m_fetch_more\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;34m\"\"\"Fetch the next URI and update state\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nextUri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decode_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/pyhive/presto.pyc\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nextUri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should not have nextUri if failed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: {u'errorCode': 16777219, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v8/type=1/part-r-00024-7eaefa5c-abc7-419a-94a5-081e2c608256.snappy.parquet (offset=0, length=500): Schema mismatch, metastore schema for row column nested has 2 fields but parquet schema has 3 fields', u'errorType': u'EXTERNAL', u'failureInfo': {u'suppressed': [], u'cause': {u'suppressed': [], u'message': u'Schema mismatch, metastore schema for row column nested has 2 fields but parquet schema has 3 fields', u'type': u'java.lang.IllegalArgumentException', u'stack': [u'com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$ParquetStructConverter.<init>(ParquetHiveRecordCursor.java:743)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createGroupConverter(ParquetHiveRecordCursor.java:718)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.access$300(ParquetHiveRecordCursor.java:99)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor$PrestoReadSupport.<init>(ParquetHiveRecordCursor.java:434)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:336)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'message': u'Error opening Hive split s3://telemetry-test-bucket/schema_evolution/v8/type=1/part-r-00024-7eaefa5c-abc7-419a-94a5-081e2c608256.snappy.parquet (offset=0, length=500): Schema mismatch, metastore schema for row column nested has 2 fields but parquet schema has 3 fields', u'type': u'com.facebook.presto.spi.PrestoException', u'stack': [u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.createParquetRecordReader(ParquetHiveRecordCursor.java:382)', u'com.facebook.presto.hive.parquet.ParquetHiveRecordCursor.<init>(ParquetHiveRecordCursor.java:156)', u'com.facebook.presto.hive.parquet.ParquetRecordCursorProvider.createRecordCursor(ParquetRecordCursorProvider.java:92)', u'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:158)', u'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:88)', u'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:44)', u'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:56)', u'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:253)', u'com.facebook.presto.operator.Driver.processInternal(Driver.java:378)', u'com.facebook.presto.operator.Driver.processFor(Driver.java:301)', u'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)', u'com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:555)', u'com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:691)', u'java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)', u'java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)', u'java.lang.Thread.run(Thread.java:745)']}, u'errorName': u'HIVE_CANNOT_OPEN_SPLIT'}"
     ]
    }
   ],
   "source": [
    "print execute(cursor, \"SELECT * FROM schema_evolution_{}\".format(v))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
